{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Maze():\n",
    "    def __init__(self,maze,start,end):\n",
    "        self.maze = maze\n",
    "        self.maze_height = maze.shape[0] # Get the height of the maze (number of rows)\n",
    "        self.maze_width = maze.shape[1]  # Get the width of the maze (number of columns)\n",
    "        self.start_position = start   # Set the start position in the maze as a tuple (x, y)\n",
    "        self.goal_position = end\n",
    "\n",
    "    def show(self):\n",
    "        plt.figure(figsize=(10,10))\n",
    "\n",
    "        plt.imshow(self.maze,cmap=\"gray\")\n",
    "        plt.text(self.start_position[0], self.start_position[1], 'S', ha='center', va='center', color='red', fontsize=20)\n",
    "        plt.text(self.goal_position[0], self.goal_position[1], 'G', ha='center', va='center', color='green', fontsize=20)\n",
    "        \n",
    "        plt.xticks([]), plt.yticks([])\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 5)\n"
     ]
    }
   ],
   "source": [
    "maze_draw = np.array([\n",
    "    [1, 0, 1, 1, 1],\n",
    "    [1, 0, 1, 0, 1],\n",
    "    [1, 1, 0, 1, 1],\n",
    "    [1, 0, 0, 0, 1],\n",
    "    [1, 1, 1, 1, 1],\n",
    "])\n",
    "\n",
    "maze = Maze(maze_draw,(0,0),(maze_draw.shape[1]-1,maze_draw.shape[0]-1))\n",
    "maze.show()\n",
    "print(maze_draw.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = [(-1, 0), \n",
    "          (1, 0),   \n",
    "          (0, -1), \n",
    "          (0, 1)]\n",
    "class QAgent():\n",
    "    def __init__(self,maze,lr =0.1,gamma = 0.9,exp_s = 1,exp_e = 0.01,num = 100):\n",
    "        self.q_table = np.zeros((maze.maze_height,maze.maze_width,4))\n",
    "        self.gamma = gamma\n",
    "        self.lr = lr\n",
    "        self.exp_s = exp_s\n",
    "        self.exp_e = exp_e\n",
    "        self.num = num\n",
    "\n",
    "    def get_exploration_rate(self,current_episode):\n",
    "        exp_rate = self.exp_s*(self.exp_e/self.exp_s) **(current_episode/self.num)\n",
    "        return exp_rate\n",
    "    def get_action(self,state,current_episode):\n",
    "        exp_rate = self.get_exploration_rate(current_episode)\n",
    "        if np.random.rand() < exp_rate:\n",
    "            return np.random.randint(4) \n",
    "        else:\n",
    "            return np.argmax(self.q_table[state])\n",
    "        \n",
    "    def update_q_table(self, state, action, next_state, reward):\n",
    "\n",
    "        best_next_action = np.argmax(self.q_table[next_state])\n",
    "\n",
    "\n",
    "        current_q_value = self.q_table[state][action]\n",
    "\n",
    "\n",
    "        new_q_value = current_q_value + self.lr * (reward + self.gamma * self.q_table[next_state][best_next_action] - current_q_value)\n",
    "\n",
    "\n",
    "        self.q_table[state][action] = new_q_value  \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "goal_reward = 100\n",
    "wall_penalty = -10\n",
    "step_penalty = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def begin(agent,maze,current_episode,train = True):\n",
    "    current_state = maze.start_position\n",
    "    is_done = False\n",
    "    ep_reward = 0\n",
    "    step = 0\n",
    "    path = [current_state]\n",
    "\n",
    "    while not is_done:\n",
    "        action = agent.get_action(current_state,current_episode)\n",
    "\n",
    "        next_state = (current_state[0] + actions[action][0],current_state[1]+actions[action][1])\n",
    "\n",
    "\n",
    "        if next_state[0] <0 or next_state[0] >= 4 or next_state[1] <0 or next_state[1] >= 4 or maze.maze[next_state[1]][next_state[0]] == 0:\n",
    "            reward = wall_penalty\n",
    "            next_state = current_state\n",
    "         \n",
    "            \n",
    "        elif next_state == maze.goal_position:\n",
    "            path.append(current_state)\n",
    "            reward = goal_reward\n",
    "            is_done = True\n",
    "        else:\n",
    "            path.append(current_state)\n",
    "            reward = step_penalty\n",
    "\n",
    "        ep_reward = reward + ep_reward\n",
    "        step = step +1\n",
    "\n",
    "\n",
    "        if train == True:\n",
    "            agent.update_q_table(current_state, action, next_state, reward)\n",
    "\n",
    "            current_state = next_state\n",
    "    return ep_reward, step, path\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = QAgent(maze)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(agent, maze, num_episodes=100):\n",
    "    # Lists to store the data for plotting\n",
    "    episode_rewards = []\n",
    "    episode_steps = []\n",
    "\n",
    "    # Loop over the specified number of episodes\n",
    "    for episode in range(num_episodes):\n",
    "        episode_reward, episode_step, path = begin(agent, maze, episode, train=True)\n",
    "\n",
    "        # Store the episode's cumulative reward and the number of steps taken in their respective lists\n",
    "        episode_rewards.append(episode_reward)\n",
    "        episode_steps.append(episode_step)\n",
    "        print(f\"cuurent episode --- {episode}\")\n",
    "        print(f\"current reward {episode_reward}\")\n",
    "\n",
    "    # Plotting the data after training is completed\n",
    "    plt.figure(figsize=(10, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(episode_rewards)\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Cumulative Reward')\n",
    "    plt.title('Reward per Episode')\n",
    "\n",
    "    average_reward = sum(episode_rewards) / len(episode_rewards)\n",
    "    print(f\"The average reward is: {average_reward}\")\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(episode_steps)\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Steps Taken')\n",
    "    plt.ylim(0, 100)\n",
    "    plt.title('Steps per Episode')\n",
    "\n",
    "    average_steps = sum(episode_steps) / len(episode_steps)\n",
    "    print(f\"The average steps is: {average_steps}\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaze\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[34], line 8\u001b[0m, in \u001b[0;36mtrain_agent\u001b[0;34m(agent, maze, num_episodes)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Loop over the specified number of episodes\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m episode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_episodes):\n\u001b[0;32m----> 8\u001b[0m     episode_reward, episode_step, path \u001b[38;5;241m=\u001b[39m \u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaze\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# Store the episode's cumulative reward and the number of steps taken in their respective lists\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     episode_rewards\u001b[38;5;241m.\u001b[39mappend(episode_reward)\n",
      "Cell \u001b[0;32mIn[32], line 9\u001b[0m, in \u001b[0;36mbegin\u001b[0;34m(agent, maze, current_episode, train)\u001b[0m\n\u001b[1;32m      6\u001b[0m path \u001b[38;5;241m=\u001b[39m [current_state]\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_done:\n\u001b[0;32m----> 9\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcurrent_episode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     next_state \u001b[38;5;241m=\u001b[39m (current_state[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m actions[action][\u001b[38;5;241m0\u001b[39m],current_state[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m+\u001b[39mactions[action][\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m next_state[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m next_state[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m next_state[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m next_state[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m maze\u001b[38;5;241m.\u001b[39mmaze[next_state[\u001b[38;5;241m1\u001b[39m]][next_state[\u001b[38;5;241m0\u001b[39m]] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[0;32mIn[7], line 20\u001b[0m, in \u001b[0;36mQAgent.get_action\u001b[0;34m(self, state, current_episode)\u001b[0m\n\u001b[1;32m     18\u001b[0m exp_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_exploration_rate(current_episode)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrand() \u001b[38;5;241m<\u001b[39m exp_rate:\n\u001b[0;32m---> 20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandint\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m \n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39margmax(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_table[state])\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_agent(agent, maze, num_episodes=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_agent(agent, maze, num_episodes=1):\n",
    "    # Simulate the agent's behavior in the maze for the specified number of episodes\n",
    "    episode_reward, episode_step, path = begin(agent, maze, num_episodes, train=False)\n",
    "\n",
    "    # Print the learned path of the agent\n",
    "    print(\"Learned Path:\")\n",
    "    for row, col in path:\n",
    "        print(f\"({row}, {col})-> \", end='')\n",
    "    print(\"Goal!\")\n",
    "\n",
    "    print(\"Number of steps:\", episode_step)\n",
    "    print(\"Total reward:\", episode_reward)\n",
    "\n",
    "    # Clear the existing plot if any\n",
    "    if plt.gcf().get_axes():\n",
    "        plt.cla()\n",
    "\n",
    "    # Visualize the maze using matplotlib\n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.imshow(maze.maze, cmap='gray')\n",
    "\n",
    "    # Mark the start position (red 'S') and goal position (green 'G') in the maze\n",
    "    plt.text(maze.start_position[0], maze.start_position[1], 'S', ha='center', va='center', color='red', fontsize=20)\n",
    "    plt.text(maze.goal_position[0], maze.goal_position[1], 'G', ha='center', va='center', color='green', fontsize=20)\n",
    "\n",
    "    # Mark the agent's path with blue '#' symbols\n",
    "    for position in path:\n",
    "        plt.text(position[0], position[1], \"#\", va='center', color='blue', fontsize=20)\n",
    "\n",
    "    # Remove axis ticks and grid lines for a cleaner visualization\n",
    "    plt.xticks([]), plt.yticks([])\n",
    "    plt.grid(color='black', linewidth=2)\n",
    "    plt.show()\n",
    "\n",
    "    return episode_step, episode_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned Path:\n",
      "(0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> (0, 0)-> Goal!\n",
      "Number of steps: 332\n",
      "Total reward: -341\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(332, -341)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_agent(agent, maze, num_episodes=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
